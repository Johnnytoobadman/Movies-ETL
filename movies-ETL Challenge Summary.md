# Movies-ETL Challenge

## Background

The dataset prepared for Amazing Prime  needs to be kept updated on a daily basis. An automated pipeline needs to be createed to take in the new data, perform the appropriate transformations and load the data into existing tables.  This process can be done by refactoring existing coding from 3 files and adding the result to the PostgrewsSQL database.

## Deliverables

 - Deliverable 1: Write and ETL Function to Read three data files: wikipedia-movies.json, movies_metadata.csv, ratings.csv

 - Deliverable 2: Extract and Transform the Wikipedia Data 
 
 - Deliverable 3: Extract and Transform the Kaggle Data

 - Deliverable 4: Create the Movie Dataset
 
 ## Resources
 
 Wikipedia-movies.json (Wikipedia)
 
 movies_metadata.csv (Kaggle)
 
 ratings.csv (MovieLens)
 
 Python 3.7
 
 Jupyter Notebook
 
 pgAdmin
 
 Postgres
 
## Summary:
 
